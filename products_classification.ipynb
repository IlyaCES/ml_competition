{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PR = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>join_date</th>\n",
       "      <th>sex</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>branch_code</th>\n",
       "      <th>occupation_code</th>\n",
       "      <th>occupation_category_code</th>\n",
       "      <th>P5DA</th>\n",
       "      <th>RIBP</th>\n",
       "      <th>...</th>\n",
       "      <th>AHXO</th>\n",
       "      <th>BSTQ</th>\n",
       "      <th>FM3X</th>\n",
       "      <th>K6QO</th>\n",
       "      <th>QBOL</th>\n",
       "      <th>JWFN</th>\n",
       "      <th>JZ9D</th>\n",
       "      <th>J9JW</th>\n",
       "      <th>GHYX</th>\n",
       "      <th>ECY3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4WKQSBB</td>\n",
       "      <td>1/2/2019</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>1987</td>\n",
       "      <td>1X1H</td>\n",
       "      <td>2A7I</td>\n",
       "      <td>T4MS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CP5S02H</td>\n",
       "      <td>1/6/2019</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>1981</td>\n",
       "      <td>UAOD</td>\n",
       "      <td>2A7I</td>\n",
       "      <td>T4MS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2YKDILJ</td>\n",
       "      <td>1/6/2013</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>1991</td>\n",
       "      <td>748L</td>\n",
       "      <td>QZYX</td>\n",
       "      <td>90QI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2S9E81J</td>\n",
       "      <td>1/8/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>1990</td>\n",
       "      <td>1X1H</td>\n",
       "      <td>BP09</td>\n",
       "      <td>56SI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BHDYVFT</td>\n",
       "      <td>1/8/2019</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>1990</td>\n",
       "      <td>748L</td>\n",
       "      <td>NO3L</td>\n",
       "      <td>T4MS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID join_date sex marital_status  birth_year branch_code  \\\n",
       "0  4WKQSBB  1/2/2019   F              M        1987        1X1H   \n",
       "1  CP5S02H  1/6/2019   F              M        1981        UAOD   \n",
       "2  2YKDILJ  1/6/2013   M              U        1991        748L   \n",
       "3  2S9E81J  1/8/2019   M              M        1990        1X1H   \n",
       "4  BHDYVFT  1/8/2019   M              M        1990        748L   \n",
       "\n",
       "  occupation_code occupation_category_code  P5DA  RIBP  ...  AHXO  BSTQ  FM3X  \\\n",
       "0            2A7I                     T4MS     0     0  ...     0     0     0   \n",
       "1            2A7I                     T4MS     0     0  ...     0     0     0   \n",
       "2            QZYX                     90QI     0     0  ...     0     0     0   \n",
       "3            BP09                     56SI     0     0  ...     0     0     0   \n",
       "4            NO3L                     T4MS     0     0  ...     0     0     0   \n",
       "\n",
       "   K6QO  QBOL  JWFN  JZ9D  J9JW  GHYX  ECY3  \n",
       "0     1     0     0     0     0     0     0  \n",
       "1     1     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     1  \n",
       "3     1     0     0     0     0     0     0  \n",
       "4     0     0     0     1     1     0     0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./dataset/Train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29132 entries, 0 to 29131\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   ID                        29132 non-null  object\n",
      " 1   join_date                 29130 non-null  object\n",
      " 2   sex                       29132 non-null  object\n",
      " 3   marital_status            29132 non-null  object\n",
      " 4   birth_year                29132 non-null  int64 \n",
      " 5   branch_code               29132 non-null  object\n",
      " 6   occupation_code           29132 non-null  object\n",
      " 7   occupation_category_code  29132 non-null  object\n",
      " 8   P5DA                      29132 non-null  int64 \n",
      " 9   RIBP                      29132 non-null  int64 \n",
      " 10  8NN1                      29132 non-null  int64 \n",
      " 11  7POT                      29132 non-null  int64 \n",
      " 12  66FJ                      29132 non-null  int64 \n",
      " 13  GYSR                      29132 non-null  int64 \n",
      " 14  SOP4                      29132 non-null  int64 \n",
      " 15  RVSZ                      29132 non-null  int64 \n",
      " 16  PYUQ                      29132 non-null  int64 \n",
      " 17  LJR9                      29132 non-null  int64 \n",
      " 18  N2MW                      29132 non-null  int64 \n",
      " 19  AHXO                      29132 non-null  int64 \n",
      " 20  BSTQ                      29132 non-null  int64 \n",
      " 21  FM3X                      29132 non-null  int64 \n",
      " 22  K6QO                      29132 non-null  int64 \n",
      " 23  QBOL                      29132 non-null  int64 \n",
      " 24  JWFN                      29132 non-null  int64 \n",
      " 25  JZ9D                      29132 non-null  int64 \n",
      " 26  J9JW                      29132 non-null  int64 \n",
      " 27  GHYX                      29132 non-null  int64 \n",
      " 28  ECY3                      29132 non-null  int64 \n",
      "dtypes: int64(22), object(7)\n",
      "memory usage: 6.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train.iloc[:, -N_PR:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column P5DA has 40 ones\n",
      "Column RIBP has 1780 ones\n",
      "Column 8NN1 has 157 ones\n",
      "Column 7POT has 316 ones\n",
      "Column 66FJ has 339 ones\n",
      "Column GYSR has 4 ones\n",
      "Column SOP4 has 431 ones\n",
      "Column RVSZ has 25328 ones\n",
      "Column PYUQ has 2173 ones\n",
      "Column LJR9 has 354 ones\n",
      "Column N2MW has 838 ones\n",
      "Column AHXO has 539 ones\n",
      "Column BSTQ has 324 ones\n",
      "Column FM3X has 110 ones\n",
      "Column K6QO has 21629 ones\n",
      "Column QBOL has 6833 ones\n",
      "Column JWFN has 311 ones\n",
      "Column JZ9D has 1425 ones\n",
      "Column J9JW has 1418 ones\n",
      "Column GHYX has 902 ones\n",
      "Column ECY3 has 1102 ones\n"
     ]
    }
   ],
   "source": [
    "for col in Y_train:\n",
    "    print(f'Column {col} has {Y_train[col].sum()} ones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ProductsClusterAttribute(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=8):\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.kmeans = KMeans(self.n_clusters)\n",
    "        self.kmeans.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        cluster_predictions = self.kmeans.predict(X)\n",
    "        return np.hstack((cluster_predictions.reshape(-1, 1), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YearMonthDayAttributes(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        data_timestamps = pd.to_datetime(X['join_date']).ffill()\n",
    "        year = pd.DatetimeIndex(data_timestamps).year.values.reshape(-1, 1)\n",
    "        month = pd.DatetimeIndex(data_timestamps).month.values.reshape(-1, 1)\n",
    "        day = pd.DatetimeIndex(data_timestamps).day.values.reshape(-1, 1)\n",
    "\n",
    "        return np.hstack((year, month, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_ones_indices(row):\n",
    "    index = np.random.choice(np.where(row == 1)[0])\n",
    "    new_row = np.zeros(len(row), dtype=bool)\n",
    "    new_row[index] = True\n",
    "    return new_row\n",
    "\n",
    "def remove_ones(X):\n",
    "    products = X[:, -21:]\n",
    "    random_ones_indices = np.apply_along_axis(get_random_ones_indices, 1, products)\n",
    "    products[random_ones_indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex', 'marital_status', 'birth_year', 'branch_code', 'occupation_code',\n",
       "       'occupation_category_code', 'P5DA', 'RIBP', '8NN1', '7POT', '66FJ',\n",
       "       'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ', 'FM3X',\n",
       "       'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW', 'GHYX', 'ECY3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "    ('one_hot', OneHotEncoder(), ['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']),\n",
    "    ('year_month_day', YearMonthDayAttributes(), ['join_date']),\n",
    "    #('products_cluster_and_products', ProductsClusterAttribute(80), Y.columns)\n",
    "], remainder='passthrough', sparse_threshold=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(pd.concat([train.drop(columns_to_drop, 1), test.drop(columns_to_drop, 1)]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train.drop(columns_to_drop, 1), Y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared = pipeline.transform(X_train)\n",
    "test_prepared = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_cluster = ProductsClusterAttribute()\n",
    "products_cluster.fit(train_prepared[:, -21:])\n",
    "\n",
    "train_prepared = np.hstack((train_prepared[:, :-21], products_cluster.transform(train_prepared[:, -21:])))\n",
    "test_prepared = np.hstack((test_prepared[:, :-21], products_cluster.transform(test_prepared[:, -21:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ones(train_prepared)\n",
    "remove_ones(test_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class ProductClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, internal_classifier_class=GradientBoostingClassifier, param_grid=None, params=None, verbose=True):\n",
    "        self.internal_classifier_class = internal_classifier_class\n",
    "        self.verbose = verbose\n",
    "        self.param_grid = param_grid\n",
    "        self.params = params\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.n_products = y.shape[1]\n",
    "        self.internal_classifiers = []\n",
    "        \n",
    "        for product_index in range(self.n_products):\n",
    "            classifier = self.internal_classifier_class()\n",
    "            \n",
    "            col_to_remove = -self.n_products + product_index\n",
    "            X_train_tmp = np.delete(X, col_to_remove, 1)\n",
    "            \n",
    "            if self.params and self.params[product_index]:\n",
    "                classifier.set_params(**self.params[product_index])\n",
    "            elif self.param_grid:\n",
    "                best_params = self.grid_search_cv(classifier, X_train_tmp, y[:, product_index])\n",
    "                classifier.set_params(**best_params)\n",
    "                if self.verbose:\n",
    "                    print(f'Internal classifier {product_index} best params {best_params}')\n",
    "            \n",
    "            classifier.fit(X_train_tmp, y[:, product_index])\n",
    "            self.internal_classifiers.append(classifier)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'Internal classifier {product_index} training finished')\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def grid_search_cv(self, model, X, y):\n",
    "        grid_search = GridSearchCV(model, self.param_grid, scoring='neg_log_loss', n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        return grid_search.best_params_\n",
    "    \n",
    "    def get_internal_classifiers_params(self):\n",
    "        return [classifier.get_params() for classifier in self.internal_classifiers]\n",
    "    \n",
    "    def set_internal_classifiers(self, classifiers):\n",
    "        self.internal_classifiers = classifiers\n",
    "        self.n_products = len(classifiers)\n",
    "        \n",
    "    def predict(self, X, set_ones_from_x=True):\n",
    "        return self._make_predictions(X, set_ones_from_x=set_ones_from_x)\n",
    "    \n",
    "    def predict_proba(self, X, set_ones_from_x=True):\n",
    "        return self._make_predictions(X, proba=True, set_ones_from_x=set_ones_from_x)\n",
    "    \n",
    "    def _make_predictions(self, X, proba=False, set_ones_from_x=True):\n",
    "        predictions = []\n",
    "        \n",
    "        for product_index in range(self.n_products):\n",
    "            col_to_remove = -self.n_products + product_index\n",
    "            X_tmp = np.delete(X, col_to_remove, 1)\n",
    "            \n",
    "            if proba:\n",
    "                prediction = self.internal_classifiers[product_index].predict_proba(X_tmp)\n",
    "            else:\n",
    "                prediction = self.internal_classifiers[product_index].predict(X_tmp)\n",
    "            predictions.append(prediction[:, 1])\n",
    "        \n",
    "        predictions = np.array(predictions).T\n",
    "        if set_ones_from_x:\n",
    "            predictions[X[:, -self.n_products:] == 1] = 1\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict_proba(X)\n",
    "        return log_loss(y.reshape(-1, 1, order='F').astype('float64'),\n",
    "                        predictions.reshape(-1, 1, order='F').astype('float64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch internal_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 44.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 68.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 89.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 119.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 136.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'gamma': [1, 0],\n",
       "                         'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': [100, 150, 200, 300],\n",
       "                         'reg_alpha ': [0, 0.1, 0.3], 'subsample': [0.8, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_log_loss', verbose=2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'n_estimators': [100, 150, 200, 300],\n",
    "    'subsample': [0.8, 1],\n",
    "    'gamma': [1, 0],\n",
    "    'reg_alpha ': [0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "col_to_remove = -N_PR + product_index\n",
    "X_train_tmp = np.delete(train_prepared, col_to_remove, 1)\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid, verbose=2, n_jobs=-1, scoring='neg_log_loss', refit=True)\n",
    "grid_search.fit(X_train_tmp, Y_train.values[:, product_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 2,\n",
       " 'n_estimators': 150,\n",
       " 'reg_alpha ': 0,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_single_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def grid_search_for_one_product(model_class, param_grid, X, Y, product_index):\n",
    "    X_train_tmp = get_x_for_one_product(X, product_index)\n",
    "    y = Y[:, product_index]\n",
    "    \n",
    "    grid_search = GridSearchCV(model_class(), param_grid, verbose=2, n_jobs=-1, scoring='neg_log_loss', refit=True)\n",
    "    grid_search.fit(X_train_tmp, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "def predict_for_one_product(model, X, product_index, proba=True):\n",
    "    X_train_tmp = get_x_for_one_product(X, product_index)\n",
    "    \n",
    "    pred = model.predict_proba(X_train_tmp)[:, 1]\n",
    "    pred = np.array(pred)\n",
    "    pred[X[:, -N_PR + product_index] == 1] = 1\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def get_x_for_one_product(X, product_index):\n",
    "    col_to_remove = -N_PR + product_index\n",
    "    X_train_tmp = np.delete(X, col_to_remove, 1)\n",
    "    \n",
    "    return X_train_tmp\n",
    "\n",
    "def score_for_one_product(model, X, Y, product_index):\n",
    "    pred = predict_for_one_product(model, X, product_index)\n",
    "    \n",
    "    return log_loss(Y.values[:, product_index].astype('float64'), pred.astype('float64'))\n",
    "\n",
    "def confusion_matrix_for_one_product(model, X, Y, product_index):\n",
    "    X_tmp = get_x_for_one_product(X, product_index)\n",
    "    pred = model.predict(X_tmp)\n",
    "    \n",
    "    print(confusion_matrix(Y[:, product_index], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1152 candidates, totalling 5760 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 27.7min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 44.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 70.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 92.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 121.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 154.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 193.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 235.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5760 out of 5760 | elapsed: 281.8min finished\n"
     ]
    }
   ],
   "source": [
    "product_index = 1\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 150, 200, 300],\n",
    "    'subsample': [0.8, 1],\n",
    "    'gamma': [0.1, 0],\n",
    "    'reg_alpha ': [0, 0.1, 0.3],\n",
    "    'n_jobs ': [-1],\n",
    "    'random_state ': [42],\n",
    "    'scale_pos_weight ': [1, (Y_train.values[:, product_index] == 1).sum() / (Y_train.values[:, product_index] == 0).sum()]\n",
    "}\n",
    "\n",
    "best_classifier, best_classifier_params = grid_search_for_one_product(XGBClassifier,\n",
    "                                                                      param_grid,\n",
    "                                                                      train_prepared,\n",
    "                                                                      Y_train.values,\n",
    "                                                                      product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0,\n",
       " 'learning_rate': 0.2,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 300,\n",
       " 'n_jobs ': -1,\n",
       " 'random_state ': 42,\n",
       " 'reg_alpha ': 0,\n",
       " 'scale_pos_weight ': 1,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04463866945650593"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_for_one_product(best_classifier, train_prepared, Y_train, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04841947222509766"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_for_one_product(best_classifier, test_prepared, Y_test, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20239   253]\n",
      " [  274  1083]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_for_one_product(best_classifier, train_prepared, Y_train.values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6766   94]\n",
      " [  98  325]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_for_one_product(best_classifier, test_prepared, Y_test.values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def scores_for_models(models, product_index):\n",
    "    FTM = '{:<30}   {:<25}  {:<25}'\n",
    "    print(FTM.format(*f'model mean std performance_gain'.split()))\n",
    "    \n",
    "    \n",
    "    for model in models:\n",
    "        X_tmp = get_x_for_one_product(full_data_prepared, product_index)\n",
    "\n",
    "        score = cross_val_score(model, X_tmp, Y.values[:, product_index],\n",
    "                                scoring='neg_log_loss', verbose=0, n_jobs=-1)\n",
    "        mean = -score.mean()\n",
    "        std = score.std()\n",
    "\n",
    "        print(FTM.format(model.__class__.__name__, mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                            mean                       std                      \n",
      "SVC                              0.2676891174080283         0.15970208713752207      \n",
      "KNeighborsClassifier             0.24846642658667273        0.02067590298642904      \n",
      "XGBClassifier                    0.033255301131902254       0.0014557602670413746    \n",
      "GradientBoostingClassifier       0.03509443247733283        0.0021850642751806595    \n",
      "RandomForestClassifier           0.062193276160904996       0.011522046496813506     \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = [\n",
    "    SVC(probability=True),\n",
    "    KNeighborsClassifier(),\n",
    "    XGBClassifier(),\n",
    "    GradientBoostingClassifier(), \n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "scores_for_models(models, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators           mean                       std                        performance_gain         \n",
      "100                    0.02722047704254462        0.001827477903095133       9999.972779522957        \n",
      "150                    0.0259954789396318         0.002047083351987715       0.001224998102912822     \n",
      "200                    0.025555109019793805       0.002336230103016028       0.00044036991983799345   \n",
      "300                    0.02569496527197419        0.002450922094331987       -0.00013985625218038492  \n",
      "400                    0.026361793206192707       0.0026634932905192695      -0.0006668279342185168   \n",
      "500                    0.027005444251445715       0.0028577650710894346      -0.0006436510452530082   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42,\n",
    "}\n",
    "\n",
    "n_estimators_values = [100, 150, 200, 300, 400, 500]\n",
    "scores_for_params(XGBClassifier, base_params, 'n_estimators', n_estimators_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth              mean                       std                        performance_gain         \n",
      "1                      0.0359726379650966         0.0019947586841376013      9999.964027362035        \n",
      "2                      0.03037986979950822        0.0016555536118216213      0.005592768165588381     \n",
      "3                      0.02722047704254462        0.001827477903095133       0.0031593927569636       \n",
      "4                      0.026126902865185642       0.002268672513537371       0.0010935741773589781    \n",
      "5                      0.02557169909262625        0.0018495325228055882      0.0005552037725593921    \n",
      "6                      0.02585585598241748        0.0018788013381550066      -0.00028415688979123097  \n",
      "7                      0.02642923222964011        0.0019438095654037296      -0.0005733762472226282   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.3,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42,\n",
    "}\n",
    "\n",
    "max_depth_values = [1, 2, 3, 4, 5, 6, 7]\n",
    "scores_for_params(XGBClassifier, base_params, 'max_depth', max_depth_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight       mean                       std                        performance_gain         \n",
      "1                      0.02557169909262625        0.0018495325228055882      9999.974428300908        \n",
      "3                      0.02660581476141279        0.001761990339127031       -0.0010341156687865387   \n",
      "5                      0.02745815895508688        0.0014186574630358775      -0.0008523441936740921   \n",
      "7                      0.029218537128766004       0.0016053093150832686      -0.0017603781736791231   \n",
      "9                      0.030662990514156774       0.0014114801950817872      -0.0014444533853907697   \n",
      "11                     0.03192305625064405        0.0017036709525742792      -0.0012600657364872786   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 5,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42,\n",
    "}\n",
    "\n",
    "min_child_weight_values = [1, 3, 5, 7, 9, 11]\n",
    "scores_for_params(XGBClassifier, base_params, 'min_child_weight', min_child_weight_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma                  mean                       std                        performance_gain         \n",
      "0                      0.02557169909262625        0.0018495325228055882      9999.974428300908        \n",
      "0.3                    0.02531030940672003        0.00209804557454618        0.00026138968590622147   \n",
      "0.6                    0.025479426526305617       0.0018095276941066996      -0.00016911711958558784  \n",
      "1                      0.025795882970334694       0.001729856801156092       -0.0003164564440290771   \n",
      "2                      0.02747926876839312        0.0018351261793901389      -0.0016833857980584274   \n",
      "5                      0.031502820021404775       0.0016569462563722364      -0.0040235512530116535   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42,\n",
    "}\n",
    "\n",
    "gamma_values = [0, 0.3, 0.6, 1, 2, 5]\n",
    "scores_for_params(XGBClassifier, base_params, 'gamma', gamma_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate          mean                       std                        performance_gain         \n",
      "0.01                   0.058513440995184074       0.0005407584165690809      9999.941486559004        \n",
      "0.03                   0.0339051917713909         0.0014509961819877814      0.024608249223793177     \n",
      "0.05                   0.03112810843144953        0.0015226226280853695      0.0027770833399413664    \n",
      "0.07                   0.029281428847920194       0.0016563384982012832      0.0018466795835293363    \n",
      "0.1                    0.027525155038580336       0.0016101215008895804      0.0017562738093398582    \n",
      "0.15                   0.026060636543819637       0.0017919500049894132      0.0014645184947606991    \n",
      "0.2                    0.025579814583659117       0.0021064904430235093      0.00048082196016051987   \n",
      "0.3                    0.02569496527197419        0.002450922094331987       -0.00011515068831507297  \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 300,\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 3,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42,\n",
    "}\n",
    "\n",
    "learning_rate_values = [0.01, 0.03, 0.05, 0.07, 0.1, 0.15, 0.2, 0.3]\n",
    "scores_for_params(XGBClassifier, base_params, 'learning_rate', learning_rate_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample              mean                       std                        performance_gain         \n",
      "0.75                   0.025932026628175765       0.002400482955063481       9999.974067973371        \n",
      "0.8                    0.02644747140532393        0.0020762001700241404      -0.0005154447771481656   \n",
      "0.85                   0.026022178890539586       0.002324102443730788       0.00042529251478434435   \n",
      "0.9                    0.0257752056481378         0.0022348586287827894      0.00024697324240178717   \n",
      "0.95                   0.025537521331492723       0.0021156925687500646      0.00023768431664507664   \n",
      "1.0                    0.025579814583659117       0.0021064904430235093      -4.229325216639421e-05   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.2,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "subsample_values = [0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "scores_for_params(XGBClassifier, base_params, 'subsample', subsample_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree       mean                       std                        performance_gain         \n",
      "0.43                   0.014041724356128221       0.0014762387958161094      9999.985958275643        \n",
      "0.45                   0.014012027735046279       0.0014089091350243893      2.9696621081942015e-05   \n",
      "0.47                   0.014039129033591221       0.0015190153323775938      -2.71012985449421e-05    \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "colsample_bytree_values = [0.43, 0.45, 0.47]\n",
    "scores_for_params(XGBClassifier, base_params, 'colsample_bytree', colsample_bytree_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma                  mean                       std                        performance_gain         \n",
      "0                      0.014012027735046279       0.0014089091350243893      9999.985987972264        \n",
      "0.3                    0.01414710333366707        0.0015585534946994694      -0.00013507559862079195  \n",
      "0.6                    0.014209868955775376       0.0015537548173317028      -6.276562210830519e-05   \n",
      "1                      0.014454189474159116       0.0015218171690944817      -0.0002443205183837397   \n",
      "2                      0.01541322338782312        0.0015812520583829324      -0.0009590339136640036   \n",
      "5                      0.017358889842922765       0.0012754263723559942      -0.0019456664550996459   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.45,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "gamma_values = [0, 0.3, 0.6, 1, 2, 5]\n",
    "scores_for_params(XGBClassifier, base_params, 'gamma', gamma_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_child_weight       mean                       std                        performance_gain         \n",
      "0.01                   0.013899147643690848       0.0018337749923535342      9999.986100852357        \n",
      "0.05                   0.01386827174312944        0.001786319948504007       3.0875900561408084e-05   \n",
      "0.1                    0.013844883498463467       0.0017776129256542742      2.3388244665972432e-05   \n",
      "0.2                    0.013948261941392112       0.001602897832052705       -0.0001033784429286446   \n",
      "0.3                    0.01402806527559385        0.0016039751012628313      -7.980333420173902e-05   \n",
      "0.5                    0.014037658897000987       0.001598849008733256       -9.593621407136266e-06   \n",
      "0.7                    0.014290238423488205       0.0014266734074982793      -0.0002525795264872177   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.45,\n",
    "    'gamma': 0,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "min_child_weight_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "scores_for_params(XGBClassifier, base_params, 'min_child_weight', min_child_weight_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_alpha              mean                       std                        performance_gain         \n",
      "0.35                   0.013757388584973402       0.0013940014459754323      9999.986242611414        \n",
      "0.4                    0.013733622493375086       0.0013830909905114448      2.3766091598315456e-05   \n",
      "0.45                   0.013849381979436675       0.0014500024847267474      -0.00011575948606158855  \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.45,\n",
    "    'gamma': 0,\n",
    "    'min_child_weight': 0.2, \n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "reg_alpha_values = [0.35, 0.4, 0.45]\n",
    "scores_for_params(XGBClassifier, base_params, 'reg_alpha', reg_alpha_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_lambda             mean                       std                        performance_gain         \n",
      "0.1                    0.013873359506543444       0.0015062136002397927      9999.986126640493        \n",
      "0.5                    0.013799119143872874       0.0014431526855419351      7.424036267057038e-05    \n",
      "0.8                    0.013824560627789112       0.0014566834638338913      -2.544148391623799e-05   \n",
      "0.9                    0.013798320141437007       0.0014317108455362201      2.6240486352104714e-05   \n",
      "1                      0.013733622493375086       0.0013830909905114448      6.469764806192081e-05    \n",
      "1.5                    0.013808054150046304       0.00144359708187996        -7.44316566712172e-05    \n",
      "3                      0.014139821563303286       0.001334239528771931       -0.0003317674132569826   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.45,\n",
    "    'gamma': 0,\n",
    "    'min_child_weight': 0.2, \n",
    "    'reg_alpha': 0.4,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "reg_lambda_values = [0.1, 0.5, 0.8, 0.9, 1, 1.5, 3]\n",
    "scores_for_params(XGBClassifier, base_params, 'reg_lambda', reg_lambda_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight       mean                       std                        performance_gain         \n",
      "0.2                    0.017826499472364622       0.0012646683123018925      9999.982173500528        \n",
      "0.25                   0.016777856051140513       0.0013534114834006868      0.0010486434212241096    \n",
      "0.3                    0.01613762152616101        0.001438336615986743       0.0006402345249795043    \n",
      "0.4                    0.015323592412227896       0.0013670539938975077      0.000814029113933112     \n",
      "0.5                    0.014916649825897483       0.001463479985403116       0.00040694258633041346   \n",
      "0.8                    0.013926290398216051       0.0013768192272799525      0.0009903594276814316    \n",
      "1                      0.013733622493375086       0.0013830909905114448      0.00019266790484096484   \n",
      "1.5                    0.01379127391806879        0.0014820788642921842      -5.765142469370439e-05   \n",
      "2                      0.013896383683025596       0.0017556175750745207      -0.0001051097649568053   \n",
      "5                      0.015107391028643818       0.0016528095566665593      -0.0012110073456182216   \n"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 0.45,\n",
    "    'gamma': 0,\n",
    "    'min_child_weight': 0.2, \n",
    "    'reg_alpha': 0.4,\n",
    "    'reg_lambda': 1,\n",
    "    'n_jobs ': -1,\n",
    "    'random_state ': 42\n",
    "}\n",
    "\n",
    "scale_pos_weight_values = [0.2, 0.25, 0.3, 0.4, 0.5, 0.8, 1, 1.5, 2, 5]\n",
    "scores_for_params(XGBClassifier, base_params, 'scale_pos_weight', scale_pos_weight_values, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_for_params(model_class, base_params, param, param_values, product_index):\n",
    "    FTM = '{:<20}   {:<25}  {:<25}  {:<25}'\n",
    "    print(FTM.format(*f'{param} mean std performance_gain'.split()))\n",
    "    \n",
    "    model = model_class(**base_params)\n",
    "    \n",
    "    last_score = 10000\n",
    "    \n",
    "    for param_value in param_values:\n",
    "        model.set_params(**{param: param_value})\n",
    "        X_tmp = get_x_for_one_product(full_data_prepared, product_index)\n",
    "\n",
    "        score = cross_val_score(model, X_tmp, Y.values[:, product_index],\n",
    "                                scoring='neg_log_loss', verbose=0, n_jobs=-1)\n",
    "        mean = -score.mean()\n",
    "        std = score.std()\n",
    "\n",
    "        print(FTM.format(param_value, mean, std, last_score - mean))\n",
    "        last_score = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open('best_model_so_far.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "best_params = pickle.load(open('best_params_so_far.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters=80 cross validation started\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'full_data_prepared' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9807995a2d2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data_prepared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_data_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_data_prepared\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtrain_data_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_data_prepared' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  KFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "n_clusters_results = []\n",
    "n_clusters_to_try = [80, 85] + list(range(100, 201, 10))\n",
    "\n",
    "for n_clusters in n_clusters_to_try:\n",
    "    print(f'n_clusters={n_clusters} cross validation started')\n",
    "    \n",
    "    product_classifier = ProductClassifier(XGBClassifier, params=best_params, verbose=False)\n",
    "    k_fold = KFold(5, shuffle=True)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for train_index, val_index in k_fold.split(full_data_prepared, Y.values):\n",
    "        train_data_x = full_data_prepared[train_index, :]\n",
    "        train_data_y = Y.values[train_index, :]\n",
    "        val_data_x = full_data_prepared[val_index, :]\n",
    "        val_data_y = Y.values[val_index, :]\n",
    "        \n",
    "        products_cluster = ProductsClusterAttribute(n_clusters)\n",
    "        products_cluster.fit(train_data_y)\n",
    "        \n",
    "        train_data_x = np.hstack((train_data_x[:, :-N_PR], products_cluster.transform(train_data_x[:, -N_PR:])))\n",
    "        val_data_x = np.hstack((val_data_x[:, :-N_PR], products_cluster.transform(val_data_x[:, -N_PR:])))\n",
    "        \n",
    "        product_classifier.fit(train_data_x, train_data_y)\n",
    "        scores.append(product_classifier.score(val_data_x, val_data_y))\n",
    "        \n",
    "        print(f'n_clusters={n_clusters} training on fold finished')\n",
    "    \n",
    "    n_clusters_results.append((n_clusters, np.mean(scores), np.std(scores)))\n",
    "    print(f'n_clusters={n_clusters} cross validation finished')\n",
    "\n",
    "\n",
    "FRT = '{:<20}  {:<20}  {:<20}'\n",
    "print(FRT.format(*'n_clusters log_loss_mean log_loss_std'.split()))\n",
    "for result in n_clusters_results:\n",
    "    print(FRT.format(*result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.03624636501159102, 0.03809287349604895) (0.03636576056490623, 0.038077706038990855)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.t.interval(0.95, 4, loc=0.03716961925381999, scale=0.000743562061527577/np.sqrt(5)), \n",
    "stats.t.interval(0.95, 4, loc=0.03722173330194854, scale=0.0006893754979451699/np.sqrt(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_cluster = ProductsClusterAttribute(200)\n",
    "products_cluster.fit(Y_train.values)\n",
    "\n",
    "train_prepared_cluster = np.hstack((train_prepared[:, :-21], products_cluster.transform(train_prepared[:, -21:])))\n",
    "test_prepared_cluster = np.hstack((test_prepared[:, :-21], products_cluster.transform(test_prepared[:, -21:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = XGBClassifier(**base_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              n_jobs =-1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, random_state =42, reg_alpha=0, reg_lambda=1,\n",
       "              scale_pos_weight=1, seed=None, silent=None, subsample=1,\n",
       "              verbosity=1)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_xgb.fit(get_x_for_one_product(train_prepared_cluster, product_index), Y_train.values[:, product_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019358024466355282"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_for_one_product(best_params_xgb, test_prepared_cluster, Y_test, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019699571698539864"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_for_one_product(model.internal_classifiers[product_index], test_prepared, Y_test, product_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01320611, 0.01461401, 0.01483579, 0.01137726, 0.01621979])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-cross_val_score(best_params_xgb, train_prepared, Y_train.values[:, product_index], scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01277334, 0.01426107, 0.01489099, 0.01098458, 0.01563331])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-cross_val_score(model.internal_classifiers[product_index], train_prepared, Y_train.values[:, product_index], scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_score': 0.5,\n",
       " 'booster': 'gbtree',\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_delta_step': 0,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 1,\n",
       " 'missing': None,\n",
       " 'n_estimators': 150,\n",
       " 'n_jobs': -1,\n",
       " 'nthread': None,\n",
       " 'objective': 'binary:logistic',\n",
       " 'random_state': 0,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'seed': None,\n",
       " 'silent': None,\n",
       " 'subsample': 1,\n",
       " 'verbosity': 1,\n",
       " 'eta': 0.2}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.internal_classifiers[product_index].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = list(itertools.repeat({\n",
    "    'n_estimators': 150,\n",
    "    'max_depth': 3,\n",
    "    'n_jobs': -1\n",
    "}, N_PR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = model.get_internal_classifiers_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_classifiers = model.internal_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_prepared = pipeline.transform(train.drop(['ID', 'join_date'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ones(full_data_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_cluster = ProductsClusterAttribute(80)\n",
    "products_cluster.fit(Y.values)\n",
    "\n",
    "full_data_prepared = np.hstack((full_data_prepared[:, :-21], products_cluster.transform(full_data_prepared[:, -21:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_model = ProductClassifier(XGBClassifier, params=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal classifier 0 training finished\n",
      "Internal classifier 1 training finished\n",
      "Internal classifier 2 training finished\n",
      "Internal classifier 3 training finished\n",
      "Internal classifier 4 training finished\n",
      "Internal classifier 5 training finished\n",
      "Internal classifier 6 training finished\n",
      "Internal classifier 7 training finished\n",
      "Internal classifier 8 training finished\n",
      "Internal classifier 9 training finished\n",
      "Internal classifier 10 training finished\n",
      "Internal classifier 11 training finished\n",
      "Internal classifier 12 training finished\n",
      "Internal classifier 13 training finished\n",
      "Internal classifier 14 training finished\n",
      "Internal classifier 15 training finished\n",
      "Internal classifier 16 training finished\n",
      "Internal classifier 17 training finished\n",
      "Internal classifier 18 training finished\n",
      "Internal classifier 19 training finished\n",
      "Internal classifier 20 training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProductClassifier(internal_classifier_class=<class 'xgboost.sklearn.XGBClassifier'>,\n",
       "                  param_grid=None,\n",
       "                  params=[{'base_score': 0.5, 'booster': 'gbtree',\n",
       "                           'colsample_bylevel': 1, 'colsample_bynode': 1,\n",
       "                           'colsample_bytree': 1, 'eta': 0.2, 'gamma': 0,\n",
       "                           'learning_rate': 0.1, 'max_delta_step': 0,\n",
       "                           'max_depth': 3, 'min_child_weight': 1,\n",
       "                           'missing': None, 'n_estimators': 100, 'n_jobs': -1,\n",
       "                           'n...\n",
       "                           'colsample_bylevel': 1, 'colsample_bynode': 1,\n",
       "                           'colsample_bytree': 1, 'eta': 0.2, 'gamma': 0,\n",
       "                           'learning_rate': 0.1, 'max_delta_step': 0,\n",
       "                           'max_depth': 6, 'min_child_weight': 1,\n",
       "                           'missing': None, 'n_estimators': 200, 'n_jobs': -1,\n",
       "                           'nthread': None, 'objective': 'binary:logistic',\n",
       "                           'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1,\n",
       "                           'scale_pos_weight': 1, 'seed': None, 'silent': None,\n",
       "                           'subsample': 1, 'verbosity': 1}],\n",
       "                  verbose=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_model.fit(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02917459687055274"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_model.score(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(full_data_model, open('best_model_so_far.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_params, open('best_params_so_far.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import  XGBClassifier\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_cluster = ProductsClusterAttribute(80)\n",
    "products_cluster.fit(Y_train.values)\n",
    "\n",
    "train_prepared_cluster = np.hstack((train_prepared[:, :-21], products_cluster.transform(train_prepared[:, -21:])))\n",
    "test_prepared_cluster = np.hstack((test_prepared[:, :-21], products_cluster.transform(test_prepared[:, -21:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_model = ProductClassifier(XGBClassifier, params=list(itertools.repeat({'max_depth':2, 'n_jobs': -1, 'n_estimators': 50}, N_PR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal classifier 0 training finished\n",
      "Internal classifier 1 training finished\n",
      "Internal classifier 2 training finished\n",
      "Internal classifier 3 training finished\n",
      "Internal classifier 4 training finished\n",
      "Internal classifier 5 training finished\n",
      "Internal classifier 6 training finished\n",
      "Internal classifier 7 training finished\n",
      "Internal classifier 8 training finished\n",
      "Internal classifier 9 training finished\n",
      "Internal classifier 10 training finished\n",
      "Internal classifier 11 training finished\n",
      "Internal classifier 12 training finished\n",
      "Internal classifier 13 training finished\n",
      "Internal classifier 14 training finished\n",
      "Internal classifier 15 training finished\n",
      "Internal classifier 16 training finished\n",
      "Internal classifier 17 training finished\n",
      "Internal classifier 18 training finished\n",
      "Internal classifier 19 training finished\n",
      "Internal classifier 20 training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProductClassifier(internal_classifier_class=<class 'xgboost.sklearn.XGBClassifier'>,\n",
       "                  param_grid=None,\n",
       "                  params=[{'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth':...\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1},\n",
       "                          {'max_depth': 2, 'n_estimators': 50, 'n_jobs': -1}],\n",
       "                  verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_model.fit(train_prepared_cluster, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05096616369788112"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_model.score(test_prepared_cluster, Y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_prepared = pipeline.transform(train.drop(columns_to_drop, 1))\n",
    "remove_ones(full_data_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_cluster = ProductsClusterAttribute(80)\n",
    "products_cluster.fit(Y.values)\n",
    "\n",
    "full_data_prepared = np.hstack((full_data_prepared[:, :-21], products_cluster.transform(full_data_prepared[:, -21:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "final_model = pickle.load(open('best_model_so_far_2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductClassifier(internal_classifier_class=<class 'xgboost.sklearn.XGBClassifier'>,\n",
       "                  param_grid=None,\n",
       "                  params=[{'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_esti...\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 100, 'n_jobs': -1}],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params = list(itertools.repeat({\n",
    "    'max_depth': 2,\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1\n",
    "}, N_PR))\n",
    "\n",
    "simple_model = ProductClassifier(XGBClassifier, params=xgb_params, verbose=False)\n",
    "simple_model.fit(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_prepared = np.hstack((simple_model.predict_proba(full_data_prepared), full_data_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal classifier 0 training finished\n",
      "Internal classifier 1 training finished\n",
      "Internal classifier 2 training finished\n",
      "Internal classifier 3 training finished\n",
      "Internal classifier 4 training finished\n",
      "Internal classifier 5 training finished\n",
      "Internal classifier 6 training finished\n",
      "Internal classifier 7 training finished\n",
      "Internal classifier 8 training finished\n",
      "Internal classifier 9 training finished\n",
      "Internal classifier 10 training finished\n",
      "Internal classifier 11 training finished\n",
      "Internal classifier 12 training finished\n",
      "Internal classifier 13 training finished\n",
      "Internal classifier 14 training finished\n",
      "Internal classifier 15 training finished\n",
      "Internal classifier 16 training finished\n",
      "Internal classifier 17 training finished\n",
      "Internal classifier 18 training finished\n",
      "Internal classifier 19 training finished\n",
      "Internal classifier 20 training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProductClassifier(internal_classifier_class=<class 'xgboost.sklearn.XGBClassifier'>,\n",
       "                  param_grid=None,\n",
       "                  params=[{'base_score': 0.5, 'booster': 'gbtree',\n",
       "                           'colsample_bylevel': 1, 'colsample_bynode': 1,\n",
       "                           'colsample_bytree': 1, 'eta': 0.2, 'gamma': 0,\n",
       "                           'learning_rate': 0.1, 'max_delta_step': 0,\n",
       "                           'max_depth': 3, 'min_child_weight': 1,\n",
       "                           'missing': None, 'n_estimators': 100, 'n_jobs': -1,\n",
       "                           'n...\n",
       "                           'colsample_bylevel': 1, 'colsample_bynode': 1,\n",
       "                           'colsample_bytree': 1, 'eta': 0.2, 'gamma': 0,\n",
       "                           'learning_rate': 0.1, 'max_delta_step': 0,\n",
       "                           'max_depth': 6, 'min_child_weight': 1,\n",
       "                           'missing': None, 'n_estimators': 200, 'n_jobs': -1,\n",
       "                           'nthread': None, 'objective': 'binary:logistic',\n",
       "                           'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1,\n",
       "                           'scale_pos_weight': 1, 'seed': None, 'silent': None,\n",
       "                           'subsample': 1, 'verbosity': 1}],\n",
       "                  verbose=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011893614510613505"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.score(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal classifier 0 training finished\n",
      "Internal classifier 1 training finished\n",
      "Internal classifier 2 training finished\n",
      "Internal classifier 3 training finished\n",
      "Internal classifier 4 training finished\n",
      "Internal classifier 5 training finished\n",
      "Internal classifier 6 training finished\n",
      "Internal classifier 7 training finished\n",
      "Internal classifier 8 training finished\n",
      "Internal classifier 9 training finished\n",
      "Internal classifier 10 training finished\n",
      "Internal classifier 11 training finished\n",
      "Internal classifier 12 training finished\n",
      "Internal classifier 13 training finished\n",
      "Internal classifier 14 training finished\n",
      "Internal classifier 15 training finished\n",
      "Internal classifier 16 training finished\n",
      "Internal classifier 17 training finished\n",
      "Internal classifier 18 training finished\n",
      "Internal classifier 19 training finished\n",
      "Internal classifier 20 training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProductClassifier(internal_classifier_class=<class 'xgboost.sklearn.XGBClassifier'>,\n",
       "                  param_grid=None,\n",
       "                  params=[{'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_esti...\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1},\n",
       "                          {'learning_rate': 0.1, 'max_depth': 2,\n",
       "                           'n_estimators': 110, 'n_jobs': -1}],\n",
       "                  verbose=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_final_params = list(itertools.repeat({\n",
    "    'max_depth': 2,\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 110,\n",
    "    'learning_rate': 0.1\n",
    "}, N_PR))\n",
    "\n",
    "final_model = ProductClassifier(XGBClassifier, params=xgb_final_params)\n",
    "final_model.fit(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02923096124809288"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.score(full_data_prepared, Y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_format(data, ids, columns=Y.columns):\n",
    "    submission = pd.DataFrame(data=data, columns=Y.columns)\n",
    "    submission['ID'] = ids\n",
    "    submission = pd.melt(submission, id_vars=['ID'], value_vars=Y.columns, var_name=\"PCODE\", value_name=\"Label\")\n",
    "    submission['ID X PCODE'] = submission['ID'] + ' X ' + submission['PCODE']\n",
    "    return submission[['ID X PCODE', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./dataset/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_prepared = pipeline.transform(test.drop(columns_to_drop, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_prepared = np.hstack((test_final_prepared[:, :-21], products_cluster.transform(test_final_prepared[:, -21:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_prepared = np.hstack((simple_model.predict_proba(test_final_prepared), test_final_prepared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = final_model.predict_proba(test_final_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission_format(test_predictions, test['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('results_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inert = []\n",
    "clusters = range(2, 30)\n",
    "for n_clusters in clusters:\n",
    "    kmeans = KMeans(n_clusters)\n",
    "    kmeans.fit(train_prepared[:, -21:])\n",
    "    inert.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(clusters, inert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(X, Y):\n",
    "    new_X = []\n",
    "    new_Y = []\n",
    "    \n",
    "    for x_row, y_row in zip(X, Y):\n",
    "        ones_indices = np.where(y_row == 1)[0]\n",
    "        \n",
    "        for i in ones_indices:\n",
    "            x_new_row = np.copy(x_row)\n",
    "            x_new_row[-N_PR + i] = 0\n",
    "            \n",
    "            new_X.append(x_new_row)\n",
    "            new_Y.append(np.copy(y_row))\n",
    "    \n",
    "    return np.array(new_X), np.array(new_Y)\n",
    "\n",
    "def upsample_for_one_product(X, Y, product_index):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    \n",
    "    y = Y[:, product_index]\n",
    "    for x_row, y in zip(X, y):\n",
    "        if y == 1:\n",
    "            new_X.append(np.copy(x_row))\n",
    "            new_y.append(1)\n",
    "        else:\n",
    "            ones_indices = np.where(x_row[-N_PR:] == 1)[0]\n",
    "            for i in ones_indices:\n",
    "                x_new_row = np.copy(x_row)\n",
    "                x_new_row[-N_PR + i] = 0\n",
    "                \n",
    "                new_X.append(x_new_row)\n",
    "                new_y.append(0)\n",
    "    \n",
    "    return np.array(new_X), np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without year, date, month: mean=-0.04553042340288534 std=-0.0008292302638647557\n",
      "With year: mean=-0.04052095208559906 std=-0.0007456094676233196\n",
      "With month: mean=-0.045532192298556914 std=-0.0008314816359417681\n",
      "With day: mean=-0.04530110197255082 std=-0.0008091571176300878\n",
      "With everything: mean=-0.03942802745108379 std=-0.0006775761791297971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def test_join_data_attribute(X, Y, model):\n",
    "    data = pipeline.transform(X.drop(columns_to_drop, 1))\n",
    "    remove_ones(data)\n",
    "    model.verbose = False\n",
    "    \n",
    "    data_timestamps = pd.to_datetime(X['join_date'])\n",
    "    year = pd.DatetimeIndex(data_timestamps).year.values.reshape(-1, 1)\n",
    "    month = pd.DatetimeIndex(data_timestamps).month.values.reshape(-1, 1)\n",
    "    day = pd.DatetimeIndex(data_timestamps).day.values.reshape(-1, 1)\n",
    "    \n",
    "    data_with_year = np.hstack((year, data))\n",
    "    data_with_month = np.hstack((month, data))\n",
    "    data_with_day = np.hstack((day, data))\n",
    "    data_with_ymd = np.hstack((year, month, day, data))\n",
    "    \n",
    "    descriptions = ['Without year, month, day', 'With year',\n",
    "                    'With month', 'With day', 'With everything']\n",
    "    data_types = [data, data_with_year, data_with_month, data_with_day, data_with_ymd]\n",
    "    \n",
    "    \n",
    "    for desc, d in zip(descriptions, data_types):\n",
    "        scores = cross_val_score(model, d, Y.values, cv=7, n_jobs=-1)\n",
    "        print(f'{desc}: mean={scores.mean()} std={scores.std()}')\n",
    "    \n",
    "test_join_data_attribute(train, Y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding: mean=0.03952546360993871 std=0.0008606107670312565\n",
      "Ordinal encoding: mean=0.03977522651709635 std=0.0009113774705102566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def test_one_hot_vs_ordinal(X, Y, model):\n",
    "    data_one_hot = pipeline.transform(X.drop(columns_to_drop, 1))\n",
    "    remove_ones(data_one_hot)\n",
    "    \n",
    "    ordinal_pipeline = ColumnTransformer([\n",
    "        ('encoder', OrdinalEncoder(), ['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']),\n",
    "        ('year_month_day', YearMonthDayAttributes(), ['join_date'])\n",
    "    ], remainder='passthrough', sparse_threshold=0.)\n",
    "    \n",
    "    data_ordinal = ordinal_pipeline.fit_transform(X.drop(columns_to_drop, 1))\n",
    "    remove_ones(data_ordinal)\n",
    "    \n",
    "    for desc, data in zip(['One hot encoding', 'Ordinal encoding'], [data_one_hot, data_ordinal]):\n",
    "        scores = cross_val_score(model, data, Y.values, cv=5, n_jobs=-1)\n",
    "        print(f'{desc}: mean={scores.mean()} std={scores.std()}')\n",
    "\n",
    "test_one_hot_vs_ordinal(train, Y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                score               \n",
      "Regular model train  0.047476564132147056\n",
      "Regular model val    0.04974279389545674 \n",
      "Stack level 1 train  0.0381584146696273  \n",
      "Stack level 1 val    0.04129655125188827 \n",
      "Stack level 2 train  0.035541379614598724\n",
      "Stack level 2 val    0.03932487556157789 \n",
      "Stack level 3 train  0.03397934411240745 \n",
      "Stack level 3 val    0.03853234726125814 \n",
      "Stack level 4 train  0.03300187263145432 \n",
      "Stack level 4 val    0.0382805374592969  \n",
      "Stack level 5 train  0.0323187153214861  \n",
      "Stack level 5 val    0.038114127555309574\n",
      "Stack level 6 train  0.031755163550807604\n",
      "Stack level 6 val    0.03821453398455538 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "\n",
    "\n",
    "def test_stacking(X, Y, model_class, model_params):\n",
    "    fmt = '{:<20} {:<20}'\n",
    "    print(fmt.format('model', 'score'))\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    X_train = pipeline.transform(X_train.drop(columns_to_drop, 1))\n",
    "    X_val = pipeline.transform(X_val.drop(columns_to_drop, 1))\n",
    "    \n",
    "    remove_ones(X_train)\n",
    "    remove_ones(X_val)\n",
    "    \n",
    "    products_cluster = ProductsClusterAttribute(80)\n",
    "    products_cluster.fit(Y_train)\n",
    "\n",
    "    X_train = np.hstack((X_train[:, :-21], products_cluster.transform(X_train[:, -21:])))\n",
    "    X_val = np.hstack((X_val[:, :-21], products_cluster.transform(X_val[:, -21:])))\n",
    "    \n",
    "    #regular_model\n",
    "    regular_model = ProductClassifier(model_class, params=model_params, verbose=False)\n",
    "    regular_model.fit(X_train, Y_train)\n",
    "    print(fmt.format('Regular model train', regular_model.score(X_train, Y_train)))\n",
    "    print(fmt.format('Regular model val', regular_model.score(X_val, Y_val)))\n",
    "    \n",
    "    #stacking\n",
    "    X_train_stacked = np.hstack((regular_model.predict_proba(X_train, set_ones_from_x=False), X_train))\n",
    "    X_val_stacked = np.hstack((regular_model.predict_proba(X_val, set_ones_from_x=False), X_val))\n",
    "    \n",
    "    for level in range(1, 7):\n",
    "        stacking_model = ProductClassifier(model_class, params=model_params, verbose=False)\n",
    "        stacking_model.fit(X_train_stacked, Y_train)\n",
    "        \n",
    "        train_score = stacking_model.score(X_train_stacked, Y_train)\n",
    "        val_score = stacking_model.score(X_val_stacked, Y_val)\n",
    "        \n",
    "        print(fmt.format(f'Stack level {level} train', train_score))\n",
    "        print(fmt.format(f'Stack level {level} val', val_score))\n",
    "        \n",
    "        train_predictions = stacking_model.predict_proba(X_train_stacked, set_ones_from_x=False)\n",
    "        val_predictions = stacking_model.predict_proba(X_val_stacked, set_ones_from_x=False)\n",
    "        \n",
    "        X_train_stacked = np.hstack((train_predictions, X_train_stacked))\n",
    "        X_val_stacked = np.hstack((val_predictions, X_val_stacked))\n",
    "        \n",
    "\n",
    "xgb_params = list(itertools.repeat({\n",
    "    'max_depth': 2,\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 0.1,\n",
    "}, N_PR))\n",
    "\n",
    "test_stacking(train, Y.values, XGBClassifier, xgb_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
